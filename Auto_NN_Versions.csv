Id,Description,Filename,Hyperparameters,Resulting_hyperparameters,BA_train,BA_test,F1_train,F1_test,G_train,G_test,I_train,I_test,fpr_train,fpr_test,tpr_train,tpr_test,auc_train,auc_test,Comment
1,"First network (10 epochs) with following layers: MaxPooling with kernel=stride=2, custom layer applying formula (w*x - a) * b, sigmoid, custom layer applying sum(x) - c. Loss - BCELossWithLogits, class weigths applied. Optimizer - Adam with lr=0.0001.",Cyclone.ipynb,"(12, 0.5, 1.0, 50.0, 2.0)","[0.08904803785546497, 1.3161153018450304, 50.24693221261384, 0.15831833762993716]",0.6192789950872728,0.6158719115144524,0.3699365544167887,0.3619197482297404,7008.68551666068,1697.5238605694683,0.0384382980687339,0.0372427349839725,0.0079300431241015,0.0077147922756241,0.2464880332986472,0.2394586153045289,0.6193085762190698,0.6159539532557637,Pretty nice.
2,"Same network, doubled the batch_size.",Cyclone_bs24.ipynb,"(24, 0.5, 1.0, 50.0, 2.0)","[0.12595123010387912, 1.3041119290093375, 50.24773618785213, 0.262760888336663]",0.6192789950872728,0.6158719115144524,0.3699365544167887,0.3619197482297404,7008.68551666068,1697.5238605694683,0.0384382980687339,0.0372427349839725,0.0079300431241015,0.0077147922756241,0.2464880332986472,0.2394586153045289,0.6960426820600156,0.7002993451608417,Surprised statistics are unchanged
3,"Decreased amount of weights. w*b = d, a*b = e. W is one for all pixels here.",Cyclone_bs24_de.ipynb,"(24, 25.0, 50.0, 2.0)","[12.55361778953873, 62.36485336981382, 9.347624206306868]",0.623715351512036,0.6228852955432127,0.3206393328700486,0.3197316936836221,4060.1185351395206,1009.3247705541838,0.0222672348583906,0.0221440274364673,0.0526473406804024,0.0519909914226843,0.3000780437044745,0.2977615825091098,0.7544148300551903,0.7505666155865989,"Crazy improvement on accuracy, but loss is very high (90). With more epochs loss decreases to 10, but accuracy drops as well."
4,"#8 but inrodcued a fully connected layer instead of sum, removed c parameter",Cyclone_bs24_de_fc.ipynb,"(24, 25.0, 50.0)","[15.40937172719522, 59.354090726467895, 0.010840546009905127, -0.04448472665502701]",0.6521417580812193,0.6559604896552448,0.2920142043108432,0.2978936810431294,3513.45928480674,927.6602954500812,0.0192691475342594,0.0203523540028539,0.1556540488739818,0.1518999472902391,0.4599375650364204,0.4638209266007287,0.6452394306312254,0.6483746967305457,
5,#12 but added additional layer with N neurons.,Cyclone_bs24_de_fc_50.ipynb,"(24, 25.0, 50.0)","[26.109520871935104, 49.5311967051249, -0.3452117192169594, -1.4733872265103591, 0.5667164423573476, 1.5385731354491552]",0.4665152121028696,0.4611346270551811,0.1353761107973965,0.1331673188663583,136.43483627367925,45.89468157062851,0.000748260553449,0.0010069039396803,0.65945136559655,0.6623221045569985,0.5924817898022893,0.5845913586673608,0.4779452998830279,0.4705343805907149,
6,"Decreased amount of weights. w*b = d, a*b = e. d is individual for each pixel here.",Cyclone_bs24_de_individual_d.ipynb,"(24, 25, 50.0, 2.0)","[22.643231420511356, 53.64948422622184, 5.7163262486130435]",0.6796316962894525,0.6742157998125637,0.2437835786614472,0.2410606669345118,3817.138284505333,892.0195594767033,0.0209346387137226,0.0195704159604366,0.434571154767609,0.4324117111505103,0.793834547346514,0.7808433107756377,0.6929096044863226,0.6907566645694346,"Surprised it performed worse than #8 (multiple runs, #8 is always better)."
7,#8 but with maxpooling kernel  = 4,Cyclone_bs24_de_mpk4.ipynb,"(24, 25.0, 50.0, 2.0)","[21.436433782203608, 53.543517093495865, 5.718906042334319]",0.6671274020782736,0.6616995451834351,0.2523504430861379,0.2489143308330043,3249.378729323932,760.4982712066248,0.0178208292894652,0.0166849116105007,0.3343195975083852,0.3330298528918491,0.6685744016649323,0.6564289432587194,0.6972775425656672,0.6945369579336397,Loss around 50.
8,#8 but with maxpooling kernel  = 6,Cyclone_bs24_de_mpk6.ipynb,"(24, 25.0, 50.0, 2.0)","[21.82619591052033, 53.15656338367588, 5.701329843883094]",0.6512072280362005,0.6459095656554563,0.2491706998749252,0.2454605119229927,2736.408532346637,638.6819509261276,0.0150075055520941,0.0140123288926311,0.293579300431241,0.2922516651492645,0.5959937565036421,0.584070796460177,0.6916036563369593,0.6901932979754798,Loss around 30.
9,"Decreased amount of weights. w*b = d, a*b = e. W is one for all pixels here. Weigths are from 0 to 1.",Cyclone_bs24_de_small_weights.ipynb,"(24, 0.5, 1.0, 1.0)","[0.0896498480524493, 1.3126924946028382, 0.04631528512418042]",0.6192789950872728,0.6158719115144524,0.3699365544167887,0.3619197482297404,7008.68551666068,1697.5238605694683,0.0384382980687339,0.0372427349839725,0.0079300431241015,0.0077147922756241,0.2464880332986472,0.2394586153045289,0.6926758613477116,0.6905417321693101,
10,"Kept batch_size=24. Fixed a, b, c and taught only w.",Cyclone_bs24_fixed_a_b_c.ipynb,"(24, 0.5, 1.0, 50.0, 2.0)",[0.12003418944073936],0.5920183390863714,0.595559097255483,0.302967420626686,0.312784443525031,5366.6096781865,1416.0659146738533,0.0294325293863334,0.0310677032618221,0.0058696693818878,0.0056543198044947,0.1899063475546306,0.1967725143154607,0.7158269323202401,0.7115545203789665,"Unsuprusingly, accuracy dropped. Suprisingly, not by much."
11,"Kept batch_size=24. Fixed b, c, and taught a and w.",Cyclone_bs24_fixed_b_c.ipynb,"(24, 0.5, 1.0, 50.0, 2.0)","[0.15539169147102097, 1.2719801322792625]",0.5920183390863714,0.595559097255483,0.302967420626686,0.312784443525031,5366.6096781865,1416.0659146738533,0.0294325293863334,0.0310677032618221,0.0058696693818878,0.0056543198044947,0.1899063475546306,0.1967725143154607,0.7129507067561093,0.7087418780484563,"Slight improvement on train, but degradation on test."
12,"Kept batch_size = 24. Fixed c, and taught, a, b and w.",Cyclone_bs24_fixed_c.ipynb,"(24, 0.5, 1.0, 50.0, 2.0)","[0.15830266569985793, 1.2682778724106643, 50.07203200090141]",0.5919764128765007,0.5957954193768524,0.3027475375842405,0.3133526250516742,5348.360716470164,1418.1389501252524,0.0293324451368361,0.0311131845134982,0.0059535218016291,0.0057022377689395,0.1899063475546306,0.1972930765226444,0.7113512198239736,0.7081922308092357,"No change. Seems like b parameter can be removed. Anyway, it's possible to decrease number of parameters, which will be done later."
13,"Added individual weights for pixel, instead of single w for all.",Cyclone_bs24_individual_w.ipynb,"(24, 0.5, 1.0, 50.0, 2.0)","[0.17598899272738172, 1.614064542046558, 50.21050425612597, 0.5524104146577605]",0.6236715181488697,0.6218246023401378,0.3574946004319654,0.354872511351729,5668.026505364616,1407.0678432400446,0.031085613950973,0.0308702905493647,0.0217776712985146,0.0207963965690737,0.2691207075962539,0.2644456012493493,0.7495796285114682,0.7394546865802185,Expected BA to increase and it did.     Multiple runs yield a different result with margin of 0.2
14,Removed MaxPooling from #1.,Cyclone_bs24_no_maxpool.ipynb,"(24, 0.5, 1.0, 50.0, 2.0)","[0.11917891420175732, 1.2946903347826295, 50.287963894191776, 0.3442707706407445]",0.6192191005017433,0.6158000345677851,0.3695758166747928,0.3614931237721022,6983.1225637416455,1689.874464957597,0.0382981011086216,0.0370749114734005,0.0080498322951605,0.0078585461689587,0.2464880332986472,0.2394586153045289,0.7295248294842908,0.7255356404016438,Slight decrease from #1.
15,#1 but all weights are from 0 to 1,Cyclone_bs24_small_weights.ipynb,"(24, 0.009999999776482582, 0.019999999552965164, 1.0, 1.0)","[-0.16131398782119288, 0.2962770612739181, 1.298112454419434, 0.07948735458328011]",0.5,0.5,0.0,0.0,0,0,0,0,0.0,0.0,0.0,0.0,0.5,0.5,
16,#1 but all weights are from 0 to 1,Cyclone_small_weights.ipynb,"(12, 0.5, 1.0, 50.0, 2.0)","[0.09569555992478619, 1.3245419775175182, 50.23608200415572, 0.16856894377306814]",0.6192789950872728,0.6158719115144524,0.3699365544167887,0.3619197482297404,7008.68551666068,1697.5238605694683,0.0384382980687339,0.0372427349839725,0.0079300431241015,0.0077147922756241,0.2464880332986472,0.2394586153045289,0.6159464076106599,0.6192979287255678,
